INFO 05-12 03:38:41 [__init__.py:239] Automatically detected platform cuda.
INFO 05-12 03:38:48 [api_server.py:1043] vLLM API server version 0.8.5.post1
INFO 05-12 03:38:48 [api_server.py:1044] args: Namespace(subparser='serve', model_tag='google/gemma-1.1-2b-it', config='', host='0.0.0.0', port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='google/gemma-1.1-2b-it', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', max_model_len=None, guided_decoding_backend='auto', reasoning_parser=None, logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, disable_sliding_window=False, use_v2_block_manager=True, seed=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, additional_config=None, enable_reasoning=False, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x78f9ef4a71a0>)
INFO 05-12 03:38:57 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 05-12 03:38:57 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 05-12 03:39:04 [__init__.py:239] Automatically detected platform cuda.
INFO 05-12 03:39:08 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='google/gemma-1.1-2b-it', speculative_config=None, tokenizer='google/gemma-1.1-2b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=google/gemma-1.1-2b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 05-12 03:39:09 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7d90772b24b0>
INFO 05-12 03:39:09 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 05-12 03:39:09 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 05-12 03:39:09 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 05-12 03:39:09 [gpu_model_runner.py:1329] Starting to load model google/gemma-1.1-2b-it...
INFO 05-12 03:39:09 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 03:39:17 [weight_utils.py:281] Time spent downloading weights for google/gemma-1.1-2b-it: 7.750651 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.63it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
INFO 05-12 03:39:19 [loader.py:458] Loading weights took 1.44 seconds
INFO 05-12 03:39:19 [gpu_model_runner.py:1347] Model loading took 4.6721 GiB and 9.668783 seconds
INFO 05-12 03:39:26 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/3fb38a00f0/rank_0_0 for vLLM's torch.compile
INFO 05-12 03:39:26 [backends.py:430] Dynamo bytecode transform time: 6.71 s
[rank0]:W0512 03:39:27.772000 38 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode
INFO 05-12 03:39:29 [backends.py:136] Cache the graph of shape None for later use
INFO 05-12 03:39:49 [backends.py:148] Compiling a graph for general shape takes 22.67 s
INFO 05-12 03:39:55 [monitor.py:33] torch.compile takes 29.38 s in total
INFO 05-12 03:39:56 [kv_cache_utils.py:634] GPU KV cache size: 811,792 tokens
INFO 05-12 03:39:56 [kv_cache_utils.py:637] Maximum concurrency for 8,192 tokens per request: 99.10x
INFO 05-12 03:40:18 [gpu_model_runner.py:1686] Graph capturing finished in 22 secs, took 1.10 GiB
INFO 05-12 03:40:18 [core.py:159] init engine (profile, create kv cache, warmup model) took 59.46 seconds
INFO 05-12 03:40:18 [core_client.py:439] Core engine process 0 ready.
INFO 05-12 03:40:19 [api_server.py:1090] Starting vLLM API server on http://0.0.0.0:8000
INFO 05-12 03:40:19 [launcher.py:28] Available routes are:
INFO 05-12 03:40:19 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD
INFO 05-12 03:40:19 [launcher.py:36] Route: /docs, Methods: GET, HEAD
INFO 05-12 03:40:19 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 05-12 03:40:19 [launcher.py:36] Route: /redoc, Methods: GET, HEAD
INFO 05-12 03:40:19 [launcher.py:36] Route: /health, Methods: GET
INFO 05-12 03:40:19 [launcher.py:36] Route: /load, Methods: GET
INFO 05-12 03:40:19 [launcher.py:36] Route: /ping, Methods: GET, POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 05-12 03:40:19 [launcher.py:36] Route: /version, Methods: GET
INFO 05-12 03:40:19 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /pooling, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /score, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /rerank, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /invocations, Methods: POST
INFO 05-12 03:40:19 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     10.84.3.1:57170 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:57338 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.4:49872 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:55894 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:55896 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:37698 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44176 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:33392 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:36834 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46170 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:38238 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58486 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:45836 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:45844 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:58802 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:49594 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:49848 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:49730 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:33260 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:35900 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:40642 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42518 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42532 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:53464 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58842 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:40676 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:39434 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:49594 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 03:41:43 [logger.py:39] Received request cmpl-a4a4c3101cfe46618b97eec9d7c71ca0-0: prompt: 'Write a poem about AI', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 5559, 476, 19592, 1105, 16481], lora_request: None, prompt_adapter_request: None.
INFO 05-12 03:41:43 [async_llm.py:252] Added request cmpl-a4a4c3101cfe46618b97eec9d7c71ca0-0.
INFO:     10.84.1.7:49602 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 03:41:49 [loggers.py:111] Engine 000: Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 24.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:36188 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:34088 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:50584 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:50596 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 03:41:59 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:49092 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:41450 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:42486 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:50044 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:43190 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:53874 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:57950 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 03:42:23 [logger.py:39] Received request cmpl-94a4ead2e90046c1b46cc84a66a8bef1-0: prompt: 'Write a poem about AI', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 5559, 476, 19592, 1105, 16481], lora_request: None, prompt_adapter_request: None.
INFO 05-12 03:42:23 [async_llm.py:252] Added request cmpl-94a4ead2e90046c1b46cc84a66a8bef1-0.
INFO:     10.84.1.7:52246 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.1.7:52248 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 03:42:29 [loggers.py:111] Engine 000: Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 24.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.128.0.56:64070 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     10.84.3.1:36066 - "GET /health HTTP/1.1" 200 OK
INFO 05-12 03:42:31 [logger.py:39] Received request cmpl-f0b424534a364e25b7f4516525020656-0: prompt: 'Write a poem about AI', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 5559, 476, 19592, 1105, 16481], lora_request: None, prompt_adapter_request: None.
INFO 05-12 03:42:31 [async_llm.py:252] Added request cmpl-f0b424534a364e25b7f4516525020656-0.
INFO:     10.84.3.4:42676 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:52250 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 03:42:39 [loggers.py:111] Engine 000: Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 29.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:55226 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:51824 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:56016 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 03:42:49 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:58518 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47046 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:56670 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.1.7:56684 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:58670 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:38916 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:57684 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:52674 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:36660 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:45500 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:50740 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40678 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.1.7:40682 - "GET /metrics HTTP/1.1" 200 OK

INFO:     10.84.3.1:45108 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:34686 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:60326 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:56418 - "GET /health HTTP/1.1" 200 OK
INFO 05-12 03:56:51 [logger.py:39] Received request cmpl-a1651f7b5e7741388c474559c06e29bc-0: prompt: 'Write a poem about AI', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 5559, 476, 19592, 1105, 16481], lora_request: None, prompt_adapter_request: None.
INFO 05-12 03:56:51 [async_llm.py:252] Added request cmpl-a1651f7b5e7741388c474559c06e29bc-0.
INFO:     10.84.3.4:38644 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:55624 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     10.84.1.7:47822 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.1.7:47826 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 03:56:59 [loggers.py:111] Engine 000: Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 19.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:56804 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:39464 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 03:57:03 [logger.py:39] Received request cmpl-58416f6406544da9b029e5a856948bb1-0: prompt: 'Write a poem about AI', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 5559, 476, 19592, 1105, 16481], lora_request: None, prompt_adapter_request: None.
INFO 05-12 03:57:03 [async_llm.py:252] Added request cmpl-58416f6406544da9b029e5a856948bb1-0.
INFO:     10.128.0.58:47833 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 03:57:09 [loggers.py:111] Engine 000: Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 24.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:50668 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44880 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58256 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 03:57:19 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:34638 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:54878 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58932 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.1.7:58938 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:40480 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58336 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:48104 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:46090 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:55328 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:46464 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:38584 - "GET /metrics HTTP/1.1" 200 OK

INFO:     10.84.1.7:48064 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 04:53:07 [logger.py:39] Received request cmpl-398fd2430e9243799fa9470a86b89f26-0: prompt: 'Write a poem about AI', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 5559, 476, 19592, 1105, 16481], lora_request: None, prompt_adapter_request: None.
INFO 05-12 04:53:07 [async_llm.py:252] Added request cmpl-398fd2430e9243799fa9470a86b89f26-0.
INFO 05-12 04:53:09 [loggers.py:111] Engine 000: Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 8.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:42362 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:46350 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39854 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     10.84.1.7:56822 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 04:53:19 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:56996 - "GET /health HTTP/1.1" 200 OK
ERROR 05-12 04:53:21 [serving_completion.py:116] Error in preprocessing prompt inputs
ERROR 05-12 04:53:21 [serving_completion.py:116] Traceback (most recent call last):
ERROR 05-12 04:53:21 [serving_completion.py:116]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_completion.py", line 108, in create_completion
ERROR 05-12 04:53:21 [serving_completion.py:116]     request_prompts, engine_prompts = await self._preprocess_completion(
ERROR 05-12 04:53:21 [serving_completion.py:116]                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 04:53:21 [serving_completion.py:116]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_engine.py", line 362, in _preprocess_completion
ERROR 05-12 04:53:21 [serving_completion.py:116]     request_prompts = await self._tokenize_prompt_input_or_inputs_async(
ERROR 05-12 04:53:21 [serving_completion.py:116]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 04:53:21 [serving_completion.py:116]   File "/usr/lib/python3.12/concurrent/futures/thread.py", line 59, in run
ERROR 05-12 04:53:21 [serving_completion.py:116]     result = self.fn(*self.args, **self.kwargs)
ERROR 05-12 04:53:21 [serving_completion.py:116]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 04:53:21 [serving_completion.py:116]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_engine.py", line 339, in _tokenize_prompt_input_or_inputs
ERROR 05-12 04:53:21 [serving_completion.py:116]     self._normalize_prompt_text_to_input(
ERROR 05-12 04:53:21 [serving_completion.py:116]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_engine.py", line 196, in _normalize_prompt_text_to_input
ERROR 05-12 04:53:21 [serving_completion.py:116]     return self._validate_input(request, input_ids, input_text)
ERROR 05-12 04:53:21 [serving_completion.py:116]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 04:53:21 [serving_completion.py:116]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_engine.py", line 259, in _validate_input
ERROR 05-12 04:53:21 [serving_completion.py:116]     raise ValueError(
ERROR 05-12 04:53:21 [serving_completion.py:116] ValueError: This model's maximum context length is 8192 tokens. However, you requested 10006 tokens (6 in the messages, 10000 in the completion). Please reduce the length of the messages or completion.
INFO:     10.84.3.1:59177 - "POST /v1/completions HTTP/1.1" 400 Bad Request
INFO:     10.84.3.4:57484 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:49630 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 04:53:29 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-12 04:53:30 [logger.py:39] Received request cmpl-467060d63d7e4b40ac1e0ff868ec3ef2-0: prompt: 'Write a poem about AI', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 5559, 476, 19592, 1105, 16481], lora_request: None, prompt_adapter_request: None.
INFO 05-12 04:53:30 [async_llm.py:252] Added request cmpl-467060d63d7e4b40ac1e0ff868ec3ef2-0.
INFO:     10.84.3.1:39920 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:40010 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:49640 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:10854 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 04:53:39 [loggers.py:111] Engine 000: Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 22.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:42574 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:51494 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39302 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 04:53:49 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:57614 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:45578 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:43694 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:58000 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:35656 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:43696 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:56394 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:49648 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42660 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:45416 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:54698 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54632 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:54928 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:46620 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54642 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:46374 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:38626 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:53662 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:44834 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:37108 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46568 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:44250 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:50794 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46582 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55552 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56562 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54216 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:42642 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56878 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:60468 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:51028 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:53062 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:60478 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:39642 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:49466 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:57580 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:47726 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:37120 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:44500 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:57648 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48164 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:44502 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:58326 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:45994 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59786 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:42494 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58502 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:60548 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:57824 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:35922 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:60560 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:41790 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:34670 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:36968 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:37250 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48420 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40378 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:41746 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:50520 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40382 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:56232 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:54554 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:49458 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:59530 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47508 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:50150 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:56954 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56868 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:50156 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:38106 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44830 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:60498 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:52666 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:41960 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:49650 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:57596 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44358 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:49660 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:34428 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56008 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:36760 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:48662 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:32918 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40942 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:55104 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:40414 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40944 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:33340 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:50686 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:51634 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:34760 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:50192 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:37994 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:50490 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:43282 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:37996 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:54018 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:39904 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42236 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:49212 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:49038 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54882 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:32784 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:60126 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54898 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:54010 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:49796 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59522 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:53854 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:55668 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:37402 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:35816 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:42838 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:37406 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:37926 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:51550 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:51298 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:57730 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:43998 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40030 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:33606 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:60464 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40034 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:38028 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48356 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:36036 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:49946 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:40552 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:53750 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:56006 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47244 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:53760 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:41162 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:38720 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.128.0.56:47633 - "OPTIONS /v1/chat/completions HTTP/1.1" 200 OK
INFO 05-12 05:01:19 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 05-12 05:01:19 [logger.py:39] Received request chatcmpl-27bbd3ccc97c4c4aab855b56f102d9b6: prompt: '<bos><start_of_turn>user\nWhat is the capital of France?<end_of_turn>\n<start_of_turn>model\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     10.128.0.56:47633 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 05-12 05:01:19 [async_llm.py:252] Added request chatcmpl-27bbd3ccc97c4c4aab855b56f102d9b6.
INFO:     10.84.1.7:44544 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:46750 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:42746 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:60686 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 05:01:29 [loggers.py:111] Engine 000: Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 2.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:54770 - "GET /health HTTP/1.1" 200 OK
ERROR 05-12 05:01:31 [chat_utils.py:1217] An error occurred in `transformers` while applying chat template
ERROR 05-12 05:01:31 [chat_utils.py:1217] Traceback (most recent call last):
ERROR 05-12 05:01:31 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/chat_utils.py", line 1203, in apply_hf_chat_template
ERROR 05-12 05:01:31 [chat_utils.py:1217]     return tokenizer.apply_chat_template(
ERROR 05-12 05:01:31 [chat_utils.py:1217]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:01:31 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py", line 1695, in apply_chat_template
ERROR 05-12 05:01:31 [chat_utils.py:1217]     rendered_chat = compiled_template.render(
ERROR 05-12 05:01:31 [chat_utils.py:1217]                     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:01:31 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 1295, in render
ERROR 05-12 05:01:31 [chat_utils.py:1217]     self.environment.handle_exception()
ERROR 05-12 05:01:31 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 942, in handle_exception
ERROR 05-12 05:01:31 [chat_utils.py:1217]     raise rewrite_traceback_stack(source=source)
ERROR 05-12 05:01:31 [chat_utils.py:1217]   File "<template>", line 1, in top-level template code
ERROR 05-12 05:01:31 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/jinja2/sandbox.py", line 401, in call
ERROR 05-12 05:01:31 [chat_utils.py:1217]     return __context.call(__obj, *args, **kwargs)
ERROR 05-12 05:01:31 [chat_utils.py:1217]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:01:31 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/transformers/utils/chat_template_utils.py", line 419, in raise_exception
ERROR 05-12 05:01:31 [chat_utils.py:1217]     raise jinja2.exceptions.TemplateError(message)
ERROR 05-12 05:01:31 [chat_utils.py:1217] jinja2.exceptions.TemplateError: Conversation roles must alternate user/assistant/user/assistant/...
ERROR 05-12 05:01:31 [serving_chat.py:200] Error in preprocessing prompt inputs
ERROR 05-12 05:01:31 [serving_chat.py:200] Traceback (most recent call last):
ERROR 05-12 05:01:31 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/chat_utils.py", line 1203, in apply_hf_chat_template
ERROR 05-12 05:01:31 [serving_chat.py:200]     return tokenizer.apply_chat_template(
ERROR 05-12 05:01:31 [serving_chat.py:200]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:01:31 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py", line 1695, in apply_chat_template
ERROR 05-12 05:01:31 [serving_chat.py:200]     rendered_chat = compiled_template.render(
ERROR 05-12 05:01:31 [serving_chat.py:200]                     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:01:31 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 1295, in render
ERROR 05-12 05:01:31 [serving_chat.py:200]     self.environment.handle_exception()
ERROR 05-12 05:01:31 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 942, in handle_exception
ERROR 05-12 05:01:31 [serving_chat.py:200]     raise rewrite_traceback_stack(source=source)
ERROR 05-12 05:01:31 [serving_chat.py:200]   File "<template>", line 1, in top-level template code
ERROR 05-12 05:01:31 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/jinja2/sandbox.py", line 401, in call
ERROR 05-12 05:01:31 [serving_chat.py:200]     return __context.call(__obj, *args, **kwargs)
ERROR 05-12 05:01:31 [serving_chat.py:200]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:01:31 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/transformers/utils/chat_template_utils.py", line 419, in raise_exception
ERROR 05-12 05:01:31 [serving_chat.py:200]     raise jinja2.exceptions.TemplateError(message)
ERROR 05-12 05:01:31 [serving_chat.py:200] jinja2.exceptions.TemplateError: Conversation roles must alternate user/assistant/user/assistant/...
ERROR 05-12 05:01:31 [serving_chat.py:200]
ERROR 05-12 05:01:31 [serving_chat.py:200] The above exception was the direct cause of the following exception:
ERROR 05-12 05:01:31 [serving_chat.py:200]
ERROR 05-12 05:01:31 [serving_chat.py:200] Traceback (most recent call last):
ERROR 05-12 05:01:31 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py", line 183, in create_chat_completion
ERROR 05-12 05:01:31 [serving_chat.py:200]     ) = await self._preprocess_chat(
ERROR 05-12 05:01:31 [serving_chat.py:200]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:01:31 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_engine.py", line 427, in _preprocess_chat
ERROR 05-12 05:01:31 [serving_chat.py:200]     request_prompt = apply_hf_chat_template(
ERROR 05-12 05:01:31 [serving_chat.py:200]                      ^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:01:31 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/chat_utils.py", line 1219, in apply_hf_chat_template
ERROR 05-12 05:01:31 [serving_chat.py:200]     raise ValueError from e
ERROR 05-12 05:01:31 [serving_chat.py:200] ValueError
INFO:     10.84.1.7:60690 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
INFO:     10.84.3.4:46570 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:60692 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:01:39 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:53558 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:59736 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59758 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:41920 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:52466 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40786 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 05:01:59 [logger.py:39] Received request chatcmpl-ef2af6b5f0254c11ae511274bb1281c8: prompt: '<bos><start_of_turn>user\nWhat is the capital of France?<end_of_turn>\n<start_of_turn>model\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:40800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 05-12 05:01:59 [async_llm.py:252] Added request chatcmpl-ef2af6b5f0254c11ae511274bb1281c8.
INFO 05-12 05:01:59 [loggers.py:111] Engine 000: Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:35082 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:37004 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40814 - "GET /metrics HTTP/1.1" 200 OK
ERROR 05-12 05:02:08 [chat_utils.py:1217] An error occurred in `transformers` while applying chat template
ERROR 05-12 05:02:08 [chat_utils.py:1217] Traceback (most recent call last):
ERROR 05-12 05:02:08 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/chat_utils.py", line 1203, in apply_hf_chat_template
ERROR 05-12 05:02:08 [chat_utils.py:1217]     return tokenizer.apply_chat_template(
ERROR 05-12 05:02:08 [chat_utils.py:1217]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:02:08 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py", line 1695, in apply_chat_template
ERROR 05-12 05:02:08 [chat_utils.py:1217]     rendered_chat = compiled_template.render(
ERROR 05-12 05:02:08 [chat_utils.py:1217]                     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:02:08 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 1295, in render
ERROR 05-12 05:02:08 [chat_utils.py:1217]     self.environment.handle_exception()
ERROR 05-12 05:02:08 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 942, in handle_exception
ERROR 05-12 05:02:08 [chat_utils.py:1217]     raise rewrite_traceback_stack(source=source)
ERROR 05-12 05:02:08 [chat_utils.py:1217]   File "<template>", line 1, in top-level template code
ERROR 05-12 05:02:08 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/jinja2/sandbox.py", line 401, in call
ERROR 05-12 05:02:08 [chat_utils.py:1217]     return __context.call(__obj, *args, **kwargs)
ERROR 05-12 05:02:08 [chat_utils.py:1217]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:02:08 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/transformers/utils/chat_template_utils.py", line 419, in raise_exception
ERROR 05-12 05:02:08 [chat_utils.py:1217]     raise jinja2.exceptions.TemplateError(message)
ERROR 05-12 05:02:08 [chat_utils.py:1217] jinja2.exceptions.TemplateError: Conversation roles must alternate user/assistant/user/assistant/...
ERROR 05-12 05:02:08 [serving_chat.py:200] Error in preprocessing prompt inputs
ERROR 05-12 05:02:08 [serving_chat.py:200] Traceback (most recent call last):
ERROR 05-12 05:02:08 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/chat_utils.py", line 1203, in apply_hf_chat_template
ERROR 05-12 05:02:08 [serving_chat.py:200]     return tokenizer.apply_chat_template(
ERROR 05-12 05:02:08 [serving_chat.py:200]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:02:08 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py", line 1695, in apply_chat_template
ERROR 05-12 05:02:08 [serving_chat.py:200]     rendered_chat = compiled_template.render(
ERROR 05-12 05:02:08 [serving_chat.py:200]                     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:02:08 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 1295, in render
ERROR 05-12 05:02:08 [serving_chat.py:200]     self.environment.handle_exception()
ERROR 05-12 05:02:08 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 942, in handle_exception
ERROR 05-12 05:02:08 [serving_chat.py:200]     raise rewrite_traceback_stack(source=source)
ERROR 05-12 05:02:08 [serving_chat.py:200]   File "<template>", line 1, in top-level template code
ERROR 05-12 05:02:08 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/jinja2/sandbox.py", line 401, in call
ERROR 05-12 05:02:08 [serving_chat.py:200]     return __context.call(__obj, *args, **kwargs)
ERROR 05-12 05:02:08 [serving_chat.py:200]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:02:08 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/transformers/utils/chat_template_utils.py", line 419, in raise_exception
ERROR 05-12 05:02:08 [serving_chat.py:200]     raise jinja2.exceptions.TemplateError(message)
ERROR 05-12 05:02:08 [serving_chat.py:200] jinja2.exceptions.TemplateError: Conversation roles must alternate user/assistant/user/assistant/...
ERROR 05-12 05:02:08 [serving_chat.py:200]
ERROR 05-12 05:02:08 [serving_chat.py:200] The above exception was the direct cause of the following exception:
ERROR 05-12 05:02:08 [serving_chat.py:200]
ERROR 05-12 05:02:08 [serving_chat.py:200] Traceback (most recent call last):
ERROR 05-12 05:02:08 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py", line 183, in create_chat_completion
ERROR 05-12 05:02:08 [serving_chat.py:200]     ) = await self._preprocess_chat(
ERROR 05-12 05:02:08 [serving_chat.py:200]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:02:08 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_engine.py", line 427, in _preprocess_chat
ERROR 05-12 05:02:08 [serving_chat.py:200]     request_prompt = apply_hf_chat_template(
ERROR 05-12 05:02:08 [serving_chat.py:200]                      ^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:02:08 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/chat_utils.py", line 1219, in apply_hf_chat_template
ERROR 05-12 05:02:08 [serving_chat.py:200]     raise ValueError from e
ERROR 05-12 05:02:08 [serving_chat.py:200] ValueError
INFO:     10.84.1.7:53006 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
INFO 05-12 05:02:09 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:33290 - "GET /health HTTP/1.1" 200 OK
/usr/local/lib/python3.12/dist-packages/prometheus_client/metrics.py:250: RuntimeWarning: coroutine 'AsyncMultiModalItemTracker.all_mm_data' was never awaited
yield Sample(suffix, dict(series_labels + list(sample_labels.items())), value, timestamp, exemplar)
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
INFO:     10.84.3.4:34214 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:02:19 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.1.7:55378 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:60228 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58518 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:56496 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:50632 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:42348 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:56504 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:52562 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:39098 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:50990 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55928 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56690 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:47146 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:52596 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:60842 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:47156 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:42204 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:37688 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:43812 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:51536 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:46440 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41070 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:34490 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:52812 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41078 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55602 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:43304 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42880 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:39244 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44234 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:34508 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:47330 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:40308 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:34516 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:49792 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56952 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:48716 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:39608 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:38856 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58452 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:51606 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56528 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58458 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.128.0.57:38261 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
INFO:     10.84.3.1:60714 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:35890 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:57410 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:36762 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:54646 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:38078 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:42008 - "GET /health HTTP/1.1" 200 OK
INFO:     10.128.0.57:18853 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
INFO:     10.84.3.4:52632 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:38082 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:49462 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:45450 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59442 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:37798 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:46010 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41926 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:44490 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44874 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41932 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:39716 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56704 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40984 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:43358 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58148 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42598 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:46242 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47918 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42614 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:47222 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:35210 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:47966 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:47626 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58660 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46748 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:51930 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48160 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:06:32 [logger.py:39] Received request chatcmpl-8337f88e85be4428abc994173ef042b1: prompt: '<bos><start_of_turn>user\nWhat is the capital of France?<end_of_turn>\n<start_of_turn>model\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     10.128.0.56:3377 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 05-12 05:06:32 [async_llm.py:252] Added request chatcmpl-8337f88e85be4428abc994173ef042b1.
INFO:     10.84.1.7:46762 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:06:39 [loggers.py:111] Engine 000: Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 2.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
ERROR 05-12 05:06:40 [chat_utils.py:1217] An error occurred in `transformers` while applying chat template
ERROR 05-12 05:06:40 [chat_utils.py:1217] Traceback (most recent call last):
ERROR 05-12 05:06:40 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/chat_utils.py", line 1203, in apply_hf_chat_template
ERROR 05-12 05:06:40 [chat_utils.py:1217]     return tokenizer.apply_chat_template(
ERROR 05-12 05:06:40 [chat_utils.py:1217]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:06:40 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py", line 1695, in apply_chat_template
ERROR 05-12 05:06:40 [chat_utils.py:1217]     rendered_chat = compiled_template.render(
ERROR 05-12 05:06:40 [chat_utils.py:1217]                     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:06:40 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 1295, in render
ERROR 05-12 05:06:40 [chat_utils.py:1217]     self.environment.handle_exception()
ERROR 05-12 05:06:40 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 942, in handle_exception
ERROR 05-12 05:06:40 [chat_utils.py:1217]     raise rewrite_traceback_stack(source=source)
ERROR 05-12 05:06:40 [chat_utils.py:1217]   File "<template>", line 1, in top-level template code
ERROR 05-12 05:06:40 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/jinja2/sandbox.py", line 401, in call
ERROR 05-12 05:06:40 [chat_utils.py:1217]     return __context.call(__obj, *args, **kwargs)
ERROR 05-12 05:06:40 [chat_utils.py:1217]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:06:40 [chat_utils.py:1217]   File "/usr/local/lib/python3.12/dist-packages/transformers/utils/chat_template_utils.py", line 419, in raise_exception
ERROR 05-12 05:06:40 [chat_utils.py:1217]     raise jinja2.exceptions.TemplateError(message)
ERROR 05-12 05:06:40 [chat_utils.py:1217] jinja2.exceptions.TemplateError: Conversation roles must alternate user/assistant/user/assistant/...
ERROR 05-12 05:06:40 [serving_chat.py:200] Error in preprocessing prompt inputs
ERROR 05-12 05:06:40 [serving_chat.py:200] Traceback (most recent call last):
ERROR 05-12 05:06:40 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/chat_utils.py", line 1203, in apply_hf_chat_template
ERROR 05-12 05:06:40 [serving_chat.py:200]     return tokenizer.apply_chat_template(
ERROR 05-12 05:06:40 [serving_chat.py:200]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:06:40 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py", line 1695, in apply_chat_template
ERROR 05-12 05:06:40 [serving_chat.py:200]     rendered_chat = compiled_template.render(
ERROR 05-12 05:06:40 [serving_chat.py:200]                     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:06:40 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 1295, in render
ERROR 05-12 05:06:40 [serving_chat.py:200]     self.environment.handle_exception()
ERROR 05-12 05:06:40 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/jinja2/environment.py", line 942, in handle_exception
ERROR 05-12 05:06:40 [serving_chat.py:200]     raise rewrite_traceback_stack(source=source)
ERROR 05-12 05:06:40 [serving_chat.py:200]   File "<template>", line 1, in top-level template code
ERROR 05-12 05:06:40 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/jinja2/sandbox.py", line 401, in call
ERROR 05-12 05:06:40 [serving_chat.py:200]     return __context.call(__obj, *args, **kwargs)
ERROR 05-12 05:06:40 [serving_chat.py:200]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:06:40 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/transformers/utils/chat_template_utils.py", line 419, in raise_exception
ERROR 05-12 05:06:40 [serving_chat.py:200]     raise jinja2.exceptions.TemplateError(message)
ERROR 05-12 05:06:40 [serving_chat.py:200] jinja2.exceptions.TemplateError: Conversation roles must alternate user/assistant/user/assistant/...
ERROR 05-12 05:06:40 [serving_chat.py:200]
ERROR 05-12 05:06:40 [serving_chat.py:200] The above exception was the direct cause of the following exception:
ERROR 05-12 05:06:40 [serving_chat.py:200]
ERROR 05-12 05:06:40 [serving_chat.py:200] Traceback (most recent call last):
ERROR 05-12 05:06:40 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py", line 183, in create_chat_completion
ERROR 05-12 05:06:40 [serving_chat.py:200]     ) = await self._preprocess_chat(
ERROR 05-12 05:06:40 [serving_chat.py:200]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:06:40 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_engine.py", line 427, in _preprocess_chat
ERROR 05-12 05:06:40 [serving_chat.py:200]     request_prompt = apply_hf_chat_template(
ERROR 05-12 05:06:40 [serving_chat.py:200]                      ^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-12 05:06:40 [serving_chat.py:200]   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/chat_utils.py", line 1219, in apply_hf_chat_template
ERROR 05-12 05:06:40 [serving_chat.py:200]     raise ValueError from e
ERROR 05-12 05:06:40 [serving_chat.py:200] ValueError
INFO:     10.84.1.7:39020 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
INFO:     10.84.3.1:50266 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:34920 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:06:49 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.1.7:58160 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:39782 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:55766 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59346 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:39166 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58780 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59348 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:57522 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58034 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59422 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:39176 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:55742 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54336 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:43254 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:39850 - "GET /metrics HTTP/1.1" 200 OK
Exception ignored in: <coroutine object AsyncMultiModalItemTracker.all_mm_data at 0x78f9ec42b890>
Traceback (most recent call last):
File "<string>", line 1, in <lambda>
KeyError: '__import__'
Exception ignored in: <coroutine object AsyncMultiModalItemTracker.all_mm_data at 0x78f9ec42b890>
Traceback (most recent call last):
File "<string>", line 1, in <lambda>
KeyError: '__import__'
INFO:     10.84.1.7:54342 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:37960 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:41824 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:47756 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:54186 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:45322 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42360 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:33808 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:55990 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42364 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:44496 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48914 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54704 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:49004 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48230 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41266 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:53952 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48550 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41268 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.128.0.56:38014 - "OPTIONS /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:08:40 [logger.py:39] Received request cmpl-6adf0de411654e3fbb5a05e863574f69-0: prompt: 'Human: What is the capital of France?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.128.0.56:38014 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:08:40 [async_llm.py:252] Added request cmpl-6adf0de411654e3fbb5a05e863574f69-0.
INFO:     10.84.3.1:49934 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:59778 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:08:49 [loggers.py:111] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.1.7:38386 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:51904 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:54448 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:45402 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 05:08:59 [logger.py:39] Received request cmpl-fa03f6c705624974bd137f008bf08037-0: prompt: 'Human: What is the capital of France?\nHuman: What is the capital of Spain?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 14034, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.128.0.57:48423 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:08:59 [async_llm.py:252] Added request cmpl-fa03f6c705624974bd137f008bf08037-0.
INFO 05-12 05:08:59 [loggers.py:111] Engine 000: Avg prompt throughput: 2.3 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:56874 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:33704 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:45412 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:09:09 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:55270 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:43476 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:09:19 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.1.7:33480 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:36118 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:53578 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39396 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:51202 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:51104 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39400 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:59900 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44146 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:53446 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:41880 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:51174 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:44830 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:52350 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44638 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:44846 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:34608 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:38388 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:38554 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:10:20 [logger.py:39] Received request cmpl-c89fc51021394c2493103ce38fec609a-0: prompt: 'Human: What is the capital of France?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:38568 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:10:20 [async_llm.py:252] Added request cmpl-c89fc51021394c2493103ce38fec609a-0.
INFO:     10.84.3.1:40260 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:33972 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39584 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 05:10:29 [loggers.py:111] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 3.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     10.84.3.1:44952 - "GET /health HTTP/1.1" 200 OK
INFO 05-12 05:10:31 [logger.py:39] Received request cmpl-51bd82047b564a01971b9e6a9778d2be-0: prompt: 'Human: What is the capital of France?\nHuman: What is the capital of Spain?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 14034, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.128.0.58:55522 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:10:31 [async_llm.py:252] Added request cmpl-51bd82047b564a01971b9e6a9778d2be-0.
INFO:     10.84.3.4:42526 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41336 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:10:39 [loggers.py:111] Engine 000: Avg prompt throughput: 2.3 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.0%
INFO:     10.84.3.1:49440 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:59104 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:10:49 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.0%
INFO:     10.84.1.7:41358 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55796 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58402 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40832 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:38840 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:37924 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:35146 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:43302 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56646 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:53832 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:46782 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:60862 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:57528 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:46596 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:60928 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42758 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:37228 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:34236 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46964 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:37974 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:46190 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:32784 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:37278 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:34968 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:32888 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:36490 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:54536 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39756 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:59150 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47728 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39172 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:48164 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47514 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54580 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:36540 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:40050 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:12:47 [logger.py:39] Received request cmpl-bb2e25ac17f44376b8b664cde54d144d-0: prompt: 'Human: What is the capital of France?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:47124 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:12:47 [async_llm.py:252] Added request cmpl-bb2e25ac17f44376b8b664cde54d144d-0.
INFO 05-12 05:12:49 [loggers.py:111] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 3.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.0%
INFO:     10.84.1.7:47128 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:36336 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48194 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:12:55 [logger.py:39] Received request cmpl-0d25b521c77f4c1c9666deccb774c569-0: prompt: 'Human: What is the capital of Spain?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 14034, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:48822 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:12:55 [async_llm.py:252] Added request cmpl-0d25b521c77f4c1c9666deccb774c569-0.
INFO:     10.84.1.7:48834 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 05:12:59 [loggers.py:111] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 3.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.0%
INFO:     10.84.3.1:44284 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44686 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:49022 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:13:09 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.0%
INFO:     10.84.3.1:36982 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:54948 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:50702 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:37118 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47732 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:57340 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:32990 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:54494 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58040 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:37226 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:37966 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39676 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55520 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:39528 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:56370 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:59646 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:40330 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40450 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:50566 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:45094 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58266 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55750 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:52758 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42544 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:52712 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:34542 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:38296 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:57740 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:57804 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:47760 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:59192 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:45328 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59636 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:54666 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:41454 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:47012 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:59258 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:34260 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:57318 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:51344 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:36878 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:51172 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:53442 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:36512 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:43892 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:38634 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:53390 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59228 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:34954 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47344 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:47716 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:54966 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:55496 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39940 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:43070 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:50422 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:49366 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:47886 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:55020 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41360 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:49016 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:43488 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39538 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55102 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:40818 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:36870 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:43188 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:52772 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41784 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:47084 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:49004 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:49110 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:53608 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48792 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:34494 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:46304 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:33272 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:33492 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:55260 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:39492 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46172 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:47784 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:51586 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:39438 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:54644 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.128.0.58:10189 - "GET / HTTP/1.0" 404 Not Found
INFO:     10.84.3.4:44656 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:51606 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:36700 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48588 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:56506 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:34184 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:45760 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:50200 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:40808 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:51488 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59806 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:42180 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:46128 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:57632 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:42006 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:57054 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:47688 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:41906 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:54186 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46882 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:55290 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:53408 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41642 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:33150 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48398 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:44060 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:40788 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:51732 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41908 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:33976 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:35888 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.128.0.57:28207 - "OPTIONS /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:19:35 [logger.py:39] Received request cmpl-e865bbd9155e4577ae352e2bea591f01-0: prompt: 'Human: What is the capital of FrancE?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 87274, 235291, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.128.0.57:28207 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:19:35 [async_llm.py:252] Added request cmpl-e865bbd9155e4577ae352e2bea591f01-0.
INFO:     10.84.1.7:52914 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:19:39 [loggers.py:111] Engine 000: Avg prompt throughput: 1.4 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.0%
INFO:     10.84.3.1:37662 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44592 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:19:49 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.0%
INFO:     10.84.3.1:49422 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:40110 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:33574 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:50330 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:35694 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48104 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:43366 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:53008 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:36448 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:49774 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:49064 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:52200 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54348 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:59058 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:34076 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:45166 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:41764 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:36060 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:54172 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:48176 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:36586 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:51148 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:36968 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:35670 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54298 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55348 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47140 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:48926 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:52904 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:41234 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:33794 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:36186 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56280 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:45620 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:51722 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:45946 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:34150 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:55162 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:40236 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:49112 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:39986 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58928 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:36456 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:49638 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:49764 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55964 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:44668 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:37282 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:35292 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:35780 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47252 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46770 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:40402 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:59634 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55594 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:40796 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:46464 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42674 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:53182 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44768 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:52956 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55680 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:36412 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:36926 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:51528 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:45702 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:51488 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:38068 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56542 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:35014 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:50812 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:54154 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:51800 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:51426 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:41776 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:38228 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:51586 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:43692 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41356 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:59776 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58790 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:44406 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:55356 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:37952 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59014 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:43210 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:53042 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:51994 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:37796 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:50506 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:59614 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:39164 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:45846 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42704 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:44164 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:60800 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40200 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:46280 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:45464 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:42846 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:35396 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:37494 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:50602 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:38284 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44002 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:44924 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:35756 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47088 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:50996 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:44030 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:59888 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40260 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:48646 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:51486 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46742 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:42038 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:35446 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:26:17 [logger.py:39] Received request cmpl-419c4d9157de452bba3716094a1518d6-0: prompt: 'Human: What is the capital of France?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.128.0.57:59740 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:26:17 [async_llm.py:252] Added request cmpl-419c4d9157de452bba3716094a1518d6-0.
INFO 05-12 05:26:19 [loggers.py:111] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 4.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.0%
INFO:     10.84.3.1:51028 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:48446 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:59080 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:48650 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 05:26:29 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.0%
INFO:     10.84.3.1:33226 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:60250 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:48608 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:57280 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:38470 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:47752 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:50416 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:54596 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:36712 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:55488 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:51470 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:33780 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:33696 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:39766 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:58934 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:50478 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:34826 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58872 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:47378 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:59018 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:57134 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:60842 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:32938 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:34178 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:38722 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:46558 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:27:55 [logger.py:39] Received request cmpl-4096c5552a0b4ff1ba49fe892b76a145-0: prompt: 'Human: What is the capital of France?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:38736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:27:55 [async_llm.py:252] Added request cmpl-4096c5552a0b4ff1ba49fe892b76a145-0.
INFO:     10.84.1.7:42272 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 05:27:59 [loggers.py:111] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.0%
INFO:     10.84.3.1:48874 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:53192 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:28:03 [logger.py:39] Received request cmpl-66b4f9783bf44e72becd077efbac9f45-0: prompt: 'Human: What is the capital of France?\nAssistant:  The capital of France is Paris.\n\nThis is incorrect. The capital of France is Paris.\nHuman: What is the capital of Spain?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292, 139, 651, 6037, 576, 6081, 603, 7127, 235265, 109, 1596, 603, 22287, 235265, 714, 6037, 576, 6081, 603, 7127, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 14034, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:42276 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:28:03 [async_llm.py:252] Added request cmpl-66b4f9783bf44e72becd077efbac9f45-0.
INFO:     10.84.1.7:55010 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:28:09 [loggers.py:111] Engine 000: Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 25.0%
INFO:     10.84.3.1:38996 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:52996 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:28:19 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 25.0%
INFO:     10.84.3.1:36634 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:57524 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:39448 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:50186 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:40770 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:53008 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:28:33 [logger.py:39] Received request cmpl-34d4b5eac64243a78fd46e6c73a3e74a-0: prompt: 'Human: What is the capital of France?\nAssistant:  The capital of France is Paris.\n\nThis is incorrect. The capital of France is Paris.\nHuman: What is the capital of Spain?\nAssistant:  The capital of Spain is Madrid.\n\nThis is also incorrect. The capital of Spain is Madrid.\nHuman: What is the capital of Portugal?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292, 139, 651, 6037, 576, 6081, 603, 7127, 235265, 109, 1596, 603, 22287, 235265, 714, 6037, 576, 6081, 603, 7127, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 14034, 235336, 108, 51190, 235292, 139, 651, 6037, 576, 14034, 603, 15500, 235265, 109, 1596, 603, 1170, 22287, 235265, 714, 6037, 576, 14034, 603, 15500, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 21539, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:50190 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:28:33 [async_llm.py:252] Added request cmpl-34d4b5eac64243a78fd46e6c73a3e74a-0.
INFO:     10.84.1.7:44632 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:28:39 [loggers.py:111] Engine 000: Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 37.5%
INFO 05-12 05:28:39 [logger.py:39] Received request cmpl-77fcbef1f8194459b9b1bc116488da80-0: prompt: 'Human: What is the capital of France?\nAssistant:  The capital of France is Paris.\n\nThis is incorrect. The capital of France is Paris.\nHuman: What is the capital of Spain?\nAssistant:  The capital of Spain is Madrid.\n\nThis is also incorrect. The capital of Spain is Madrid.\nHuman: What is the capital of Portugal?\nAssistant:   The capital of Portugal is Lisbon.\n\nThis is correct. The capital of Portugal is Lisbon.\nHuman: What is the capital of Turkey?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292, 139, 651, 6037, 576, 6081, 603, 7127, 235265, 109, 1596, 603, 22287, 235265, 714, 6037, 576, 6081, 603, 7127, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 14034, 235336, 108, 51190, 235292, 139, 651, 6037, 576, 14034, 603, 15500, 235265, 109, 1596, 603, 1170, 22287, 235265, 714, 6037, 576, 14034, 603, 15500, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 21539, 235336, 108, 51190, 235292, 140, 651, 6037, 576, 21539, 603, 68696, 235265, 109, 1596, 603, 5112, 235265, 714, 6037, 576, 21539, 603, 68696, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 21355, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:44644 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:28:39 [async_llm.py:252] Added request cmpl-77fcbef1f8194459b9b1bc116488da80-0.
INFO:     10.84.3.1:55440 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:42678 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:28:49 [loggers.py:111] Engine 000: Avg prompt throughput: 11.3 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 53.3%
INFO:     10.84.3.1:55764 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:58492 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:49916 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42126 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 05:28:59 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 53.3%
INFO:     10.84.3.1:40168 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:42242 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:29:06 [logger.py:39] Received request cmpl-606c11e175e147ea996cfb5ad46905eb-0: prompt: 'Human: What is the capital of Spain?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 14034, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:38074 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:29:06 [async_llm.py:252] Added request cmpl-606c11e175e147ea996cfb5ad46905eb-0.
INFO:     10.84.1.7:38082 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:29:09 [loggers.py:111] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 3.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 53.3%
INFO:     10.84.3.1:47468 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44000 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:29:19 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 53.3%
INFO:     10.84.3.1:60646 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:46760 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:53012 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:47898 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:48146 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44518 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:50862 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55176 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47652 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:57108 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:34094 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:51324 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46608 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:40336 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58392 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:44494 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:39424 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:33832 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:53180 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:47618 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:46594 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:47836 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:45030 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:55366 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40568 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:33514 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:42054 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:60134 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:57564 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:36414 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41348 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:42146 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44432 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:44092 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:60258 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:55806 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:34406 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:38838 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:55464 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39560 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:59980 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:54976 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39108 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:45724 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:33536 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:59908 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:35962 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:51560 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:34184 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:46866 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:40426 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40088 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:53144 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:52854 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:36302 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:42876 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:60288 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:33432 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:42716 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:57218 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:48138 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:35368 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48334 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:32936 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:59320 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:41942 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58536 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:54058 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58050 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:52036 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:35534 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:46760 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:55722 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:47214 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:35784 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42588 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:44856 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:36458 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:60672 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:51972 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:50480 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:59510 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:38172 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:44896 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:31293 - "OPTIONS /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:33:57 [logger.py:39] Received request cmpl-084b676240704ebf952e61a2c7ffc319-0: prompt: 'Human: What is the capital of France?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.3.1:31293 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:33:57 [async_llm.py:252] Added request cmpl-084b676240704ebf952e61a2c7ffc319-0.
INFO:     10.84.1.7:55478 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 05:33:59 [loggers.py:111] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 53.3%
INFO:     10.84.3.1:40778 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:59910 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:57336 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:34:07 [logger.py:39] Received request cmpl-c4be0cdd6c6242ec8a84376a82912232-0: prompt: 'Human: What is the capital of France?\nAssistant:  The capital of France is Paris.\n\nThis is incorrect. The capital of France is Paris.\nHuman: What is the capital of Spain?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292, 139, 651, 6037, 576, 6081, 603, 7127, 235265, 109, 1596, 603, 22287, 235265, 714, 6037, 576, 6081, 603, 7127, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 14034, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:57350 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:34:07 [async_llm.py:252] Added request cmpl-c4be0cdd6c6242ec8a84376a82912232-0.
INFO 05-12 05:34:09 [loggers.py:111] Engine 000: Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 58.8%
INFO:     10.84.3.1:41190 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:46564 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:34:16 [logger.py:39] Received request cmpl-0a18e3f96d9541f98ad063f0dd13b90b-0: prompt: 'Human: What is the capital of France?\nAssistant:  The capital of France is Paris.\n\nThis is incorrect. The capital of France is Paris.\nHuman: What is the capital of Spain?\nAssistant:  The capital of Spain is Madrid.\n\nThis is incorrect. The capital of Spain is Madrid.\nHuman: What is the capital of Morocco?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292, 139, 651, 6037, 576, 6081, 603, 7127, 235265, 109, 1596, 603, 22287, 235265, 714, 6037, 576, 6081, 603, 7127, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 14034, 235336, 108, 51190, 235292, 139, 651, 6037, 576, 14034, 603, 15500, 235265, 109, 1596, 603, 22287, 235265, 714, 6037, 576, 14034, 603, 15500, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 52237, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:33780 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:34:16 [async_llm.py:252] Added request cmpl-0a18e3f96d9541f98ad063f0dd13b90b-0.
INFO 05-12 05:34:19 [loggers.py:111] Engine 000: Avg prompt throughput: 7.9 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.9%
INFO:     10.84.3.1:46054 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:33796 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:33048 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:34:27 [logger.py:39] Received request cmpl-7321e7902223435997c5104ad95c24ac-0: prompt: 'Human: What is the capital of France?\nAssistant:  The capital of France is Paris.\n\nThis is incorrect. The capital of France is Paris.\nHuman: What is the capital of Spain?\nAssistant:  The capital of Spain is Madrid.\n\nThis is incorrect. The capital of Spain is Madrid.\nHuman: What is the capital of Morocco?\nAssistant:   The capital of Morocco is Rabat.\n\nThis is incorrect. The capital of Morocco is Rabat.\nHuman: What is the capital of Turkey?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292, 139, 651, 6037, 576, 6081, 603, 7127, 235265, 109, 1596, 603, 22287, 235265, 714, 6037, 576, 6081, 603, 7127, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 14034, 235336, 108, 51190, 235292, 139, 651, 6037, 576, 14034, 603, 15500, 235265, 109, 1596, 603, 22287, 235265, 714, 6037, 576, 14034, 603, 15500, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 52237, 235336, 108, 51190, 235292, 140, 651, 6037, 576, 52237, 603, 168490, 235265, 109, 1596, 603, 22287, 235265, 714, 6037, 576, 52237, 603, 168490, 235265, 108, 20279, 235292, 2439, 603, 573, 6037, 576, 21355, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.3.1:59050 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:34:27 [async_llm.py:252] Added request cmpl-7321e7902223435997c5104ad95c24ac-0.
INFO:     10.84.1.7:39834 - "GET /v1/models HTTP/1.1" 200 OK
INFO 05-12 05:34:29 [loggers.py:111] Engine 000: Avg prompt throughput: 11.2 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 63.0%
INFO:     10.84.3.1:52204 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47952 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:56392 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:34:39 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 63.0%
INFO:     10.84.3.1:52414 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:59736 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:53508 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:48080 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:37042 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:33286 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:51888 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:39846 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39332 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:40134 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:42998 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:36032 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:41248 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:55858 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:41954 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:39428 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:41594 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46656 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:34064 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:38644 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:45824 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:48688 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:57928 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:52558 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:40110 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:36172 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:42724 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:35742 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47894 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:56886 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.1.7:60584 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:47454 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58222 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:35178 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:33778 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:47942 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:59722 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48638 - "GET /metrics HTTP/1.1" 200 OK

INFO:     10.128.0.58:37745 - "GET / HTTP/1.1" 404 Not Found
INFO:     10.84.1.7:60502 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:49270 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:57288 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54850 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:35078 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:53454 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:41:14 [logger.py:39] Received request cmpl-57063e5871d1449d8996ef430e61fdd2-0: prompt: 'Human: What is the capital of France?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:54862 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:41:14 [async_llm.py:252] Added request cmpl-57063e5871d1449d8996ef430e61fdd2-0.
INFO 05-12 05:41:19 [loggers.py:111] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.0%
INFO:     10.84.3.1:51534 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:58196 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:57924 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:41:29 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.0%
INFO:     10.84.1.7:38470 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:59362 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:41850 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:52292 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:52412 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48392 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:33498 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:47930 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58402 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:41:58 [logger.py:39] Received request cmpl-cebf0b8f7e4f4ed3b9260cf928ccc5f8-0: prompt: 'Human: What is the capital of France?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2439, 603, 573, 6037, 576, 6081, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.84.1.7:51436 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 05:41:58 [async_llm.py:252] Added request cmpl-cebf0b8f7e4f4ed3b9260cf928ccc5f8-0.
INFO 05-12 05:41:59 [loggers.py:111] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.0%
INFO:     10.84.1.7:51440 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:56556 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56986 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:55640 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 05:42:09 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.0%
INFO:     10.84.3.1:41476 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:43562 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:39974 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:42028 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58514 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.4:38540 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58398 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:44806 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:59628 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:42158 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:36840 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:51038 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.128.0.57:51490 - "POST /boaform/admin/formLogin HTTP/1.1" 404 Not Found
INFO:     10.84.1.7:51350 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:56290 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:60242 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:40914 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:44706 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:53640 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:48452 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48994 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:36552 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.128.0.56:9730 - "POST /cgi-bin/../../../../../../../../../../bin/sh HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "POST /cgi-bin/%252e%252e/%252e%252e/%252e%252e/%252e%252e/%252e%252e/%252e%252e/%252e%252e/bin/sh HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "POST /hello.world?%ADd+allow_url_include%3d1+%ADd+auto_prepend_file%3dphp://input HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /vendor/phpunit/phpunit/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /vendor/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /vendor/phpunit/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /vendor/phpunit/phpunit/LICENSE/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /vendor/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.84.3.1:46326 - "GET /health HTTP/1.1" 200 OK
INFO:     10.128.0.56:9730 - "GET /phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /phpunit/phpunit/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /phpunit/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /lib/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /lib/phpunit/phpunit/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /lib/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /lib/phpunit/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /lib/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.84.3.4:37070 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.128.0.56:9730 - "GET /laravel/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /www/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /ws/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /yii/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /zend/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /ws/ec/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /V2/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /tests/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /test/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /testing/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /api/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /demo/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /cms/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /crm/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /admin/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /backup/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /blog/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /workspace/drupal/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /panel/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /public/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /apps/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /app/vendor/phpunit/phpunit/src/Util/PHP/eval-stdin.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /index.php?s=/index/\think\app/invokefunction&function=call_user_func_array&vars[0]=md5&vars[1][]=Hello HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /public/index.php?s=/index/\think\app/invokefunction&function=call_user_func_array&vars[0]=md5&vars[1][]=Hello HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /index.php?lang=../../../../../../../../usr/local/lib/php/pearcmd&+config-create+/&/<?echo(md5("hi"));?>+/tmp/index1.php HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /index.php?lang=../../../../../../../../tmp/index1 HTTP/1.1" 404 Not Found
INFO:     10.128.0.56:9730 - "GET /containers/json HTTP/1.1" 404 Not Found
INFO:     10.84.3.1:38544 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:46192 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54726 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:52540 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:60138 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:40842 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:59504 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:52488 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:34890 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:45942 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:51942 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:57466 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:39528 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.3.1:48504 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:38788 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54586 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:42634 - "GET /health HTTP/1.1" 200 OK


INFO:     10.128.0.56:58096 - "OPTIONS /v1/completions HTTP/1.1" 200 OK
INFO 05-12 08:47:38 [logger.py:39] Received request cmpl-478eef67f5934a339c130cbd867c94a5-0: prompt: 'Human: test\nAssistant: Sorry, there was an error processing your request.\nHuman: heyy\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2121, 108, 51190, 235292, 26199, 235269, 1104, 729, 671, 3333, 10310, 861, 3853, 235265, 108, 20279, 235292, 693, 8053, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.128.0.56:58096 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 08:47:38 [async_llm.py:252] Added request cmpl-478eef67f5934a339c130cbd867c94a5-0.
INFO 05-12 08:47:40 [loggers.py:111] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 63.4%
INFO:     10.84.3.1:39700 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:56492 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 08:47:46 [logger.py:39] Received request cmpl-7dc0091bad4b4f568aed7923343494e7-0: prompt: 'Human: test\nAssistant: Sorry, there was an error processing your request.\nHuman: heyy\nAssistant:  I am unable to understand the context of your message. Could you please rephrase your question?\nHuman: how are you?\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2, 20279, 235292, 2121, 108, 51190, 235292, 26199, 235269, 1104, 729, 671, 3333, 10310, 861, 3853, 235265, 108, 20279, 235292, 693, 8053, 108, 51190, 235292, 139, 235285, 1144, 14321, 577, 3508, 573, 4807, 576, 861, 3969, 235265, 20337, 692, 3743, 582, 41050, 861, 2872, 235336, 108, 20279, 235292, 1368, 708, 692, 235336, 108, 51190, 235292], lora_request: None, prompt_adapter_request: None.
INFO:     10.128.0.56:18177 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 08:47:46 [async_llm.py:252] Added request cmpl-7dc0091bad4b4f568aed7923343494e7-0.
INFO 05-12 08:47:50 [loggers.py:111] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 3.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.4%
INFO:     10.84.3.1:42762 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:42462 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:38242 - "GET /metrics HTTP/1.1" 200 OK
INFO 05-12 08:48:00 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.4%
INFO:     10.84.3.1:47036 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:60546 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:46108 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.1.7:43200 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:47788 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:59304 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:54976 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:43110 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:58596 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:42572 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:48888 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:54676 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     10.84.1.7:36380 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:44712 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:45900 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.3.1:34540 - "GET /health HTTP/1.1" 200 OK
INFO:     10.84.3.4:44764 - "GET /metrics HTTP/1.1" 200 OK
INFO:     10.84.1.7:56188 - "GET /metrics HTTP/1.1" 200 OK
Total Log Lines
